import numpy as np
import argparse
import torch
import os, json
import time
from datetime import datetime
import matplotlib.pyplot as plt

from rlss_env.rlss_envs import ServerlessEnv 
from utils.dqn_agent import Agent as dqn
from traffic_generator.request_generator import PoissonGenerator, RealTraceGenerator
import utils.log_plot as lp 

now = datetime.now()

# testing the trained model
def test(args, folder_base, env_config, traffic_gen, drl_hyper_params):
    folder_name = os.path.join(folder_base, 'test')
    os.makedirs(folder_name, exist_ok=True)
    log_path = os.path.join(folder_name, 'log.json')
    if os.path.exists(log_path):
        os.remove(log_path)
        
    env_config["custom_profiling"] = True
    env_config["profiling_path"] = os.path.join(folder_base,"profiling.npz")

    env = ServerlessEnv(traffic_generator=traffic_gen,
                        render_mode=env_config["render_mode"],
                        num_service=env_config["num_service"],
                        step_interval=env_config["step_interval"],
                        num_container=env_config["num_container"],
                        cont_span=env_config["cont_span"],
                        energy_price=env_config["energy_price"],
                        ram_profit=env_config["ram_profit"],
                        cpu_profit=env_config["cpu_profit"],
                        delay_coff=env_config["delay_coff"],
                        aban_coff=env_config["aban_coff"],
                        energy_coff=env_config["energy_coff"],
                        profiling_path=env_config["profiling_path"],
                        custom_profiling=env_config["custom_profiling"])
    
    action_size = env.action_size
    state_dim = env.state_space.shape[0]
    
    
    drl_hyper_params["action_size"] = action_size
    drl_hyper_params["state_dim"] = state_dim          
        

    agent = dqn(drl_hyper_params["state_dim"],
                drl_hyper_params["action_size"], 
                drl_hyper_params["replay_buffer_size"],
                drl_hyper_params["batch_size"],
                drl_hyper_params["hidden_size"],
                drl_hyper_params["gamma"],
                drl_hyper_params["learning_rate"],
                folder_base)
    
    # Loading the Model's weights generated by the selected model
    agent.load_models()

    if args['observe'] is None:
        eps = 3
    else:
        eps = int(args['observe'])  
    
    log_data = {}
    for e in range(eps):
        done = False
        trun = False
        cum_reward = 0
        rewards = []
        state = env.reset()
        state = np.reshape(state, [state_dim])
        log_data[f"Episode {e}"] = []
        while not done and not trun:
            action = agent.get_action(state, env=env, epsilon=0)
            next_state, reward, done, trun = env.step(action)
            next_state = np.reshape(next_state, [state_dim])
            state = next_state
            rewards.append(reward)
            cum_reward += reward
            
            step_info = env.render()
            step_info["Episode"] = f"{e}/{eps}"
            log_data[f"Episode {e}"].append(step_info)
    
        
    lp.append_to_json(log_data,log_path)      
    lp.plot_log_fig(log_path, eps, env_config["step_interval"], env_config["num_service"])


# Training the model            
def train(args, folder_base, env_config, traffic_gen, drl_hyper_params):
    folder_name = os.path.join(folder_base, 'train')
    os.makedirs(folder_name, exist_ok=True)
    log_path = os.path.join(folder_name, 'log.pkl')
    open(log_path,'w').close()
    
    env_config["custom_profiling"] = False
    env_config["profiling_path"] = os.path.join(folder_base,"profiling.npz")
        
    env = ServerlessEnv(traffic_generator=traffic_gen,
                        render_mode=env_config["render_mode"],
                        num_service=env_config["num_service"],
                        step_interval=env_config["step_interval"],
                        num_container=env_config["num_container"],
                        cont_span=env_config["cont_span"],
                        energy_price=env_config["energy_price"],
                        ram_profit=env_config["ram_profit"],
                        cpu_profit=env_config["cpu_profit"],
                        delay_coff=env_config["delay_coff"],
                        aban_coff=env_config["aban_coff"],
                        energy_coff=env_config["energy_coff"],
                        profiling_path=env_config["profiling_path"],
                        custom_profiling=env_config["custom_profiling"],
                        reward_add=env_config["reward_add"])
    
    action_size = env.action_size
    state_dim = env.state_space.shape[0]
    
    drl_hyper_params["action_size"] = action_size
    drl_hyper_params["state_dim"] = state_dim    

    # Instantiating the DQN_Agent
    agent = dqn(drl_hyper_params["state_dim"],
                drl_hyper_params["action_size"], 
                drl_hyper_params["replay_buffer_size"],
                drl_hyper_params["batch_size"],
                drl_hyper_params["hidden_size"],
                drl_hyper_params["gamma"],
                drl_hyper_params["learning_rate"],
                folder_base)

    tab = {}
    avg_reward_list = []
    rewards = []
    cumulative_rewards = []
    
    eps = drl_hyper_params["episodes"]
    
    best_cum = -np.inf
    
    for e in range(eps):
        state = env.reset()
        state = np.reshape(state, [state_dim])
        done = False
        trun = False
        cum_reward = 0
        log_data = []
        while not done and not trun:
            action = agent.get_action(state=state,env=env,epsilon=drl_hyper_params["epsilon"])
            next_state, reward, done, trun = env.step(action)
            
            tab[e * env.now + env.now] = {"action": action, "reward": reward, "next_state": next_state}
            next_state = np.reshape(next_state, [state_dim])
            agent.store_transition(state, action, reward, next_state, False)
            state = next_state
            agent.learn()
            rewards.append(reward)
            cum_reward += reward
            
            step_info = env.render()
            step_info["Episode"] = f"{e}/{eps}"
            log_data.append(step_info)
            
        lp.append_to_pickle(log_data,log_path)   
        drl_hyper_params["epsilon"] = max(drl_hyper_params["eps_min"], drl_hyper_params["eps_decay"]*drl_hyper_params["epsilon"])
            
        if e % drl_hyper_params["batch_update"] == 0:
            agent.update_target_network()
            
        cumulative_rewards.append(cum_reward)
        if e > drl_hyper_params["max_env_steps"]:
            avg = np.mean(cumulative_rewards[-drl_hyper_params["max_env_steps"]:])
        else:
            avg = np.mean(cumulative_rewards)
            
        if best_cum <= cum_reward:
            best_cum = cum_reward
            print(f"Best cum reward: {cum_reward} at episode: {e}")
            agent.save_models()
            
        avg_reward_list.append(avg)
        plt.figure(2)
        plt.clf()
        rewards_t = torch.tensor(cumulative_rewards, dtype=torch.float)
        plt.title('Training...')
        plt.xlabel('Episode')
        plt.ylabel('Cumulative reward')
        plt.grid(True)
        plt.plot(rewards_t.numpy())

        if len(rewards_t) >= drl_hyper_params["max_env_steps"]:
            means = rewards_t.unfold(0, drl_hyper_params["max_env_steps"], 1).mean(1).view(-1)
            means = torch.cat((torch.zeros(drl_hyper_params["max_env_steps"]-1), means))
            plt.plot(means.numpy())
        
        plt.savefig(os.path.join(folder_name,'live_traning.png'))
        plt.close()
        
    # Plotting the reward/avg_reward
    plt.plot((np.arange(len(avg_reward_list)) + 1), avg_reward_list)
    plt.xlabel('Episodes')
    plt.ylabel('Average Reward')
    plt.title('Average Reward vs Episodes')
    plt.grid(True)
    plt.savefig(os.path.join(folder_name, 'average_rewards_{}.png'.format(args['model'])))
    plt.close()

    plt.plot(cumulative_rewards)
    plt.plot(avg_reward_list)
    plt.legend(["Reward", "{}-episode average".format(drl_hyper_params["max_env_steps"])])
    plt.grid(True)
    plt.title("Reward history")
    plt.savefig(os.path.join(folder_name, 'Live_average_rewards_{}.png'.format(args['model'])))
    plt.close()

    # Saving all sort of statistics
    with open(os.path.join(folder_name, 'action_state_information_{}.txt'.format(args['model'])), "a") as w:
        w.write(str(tab))
    with open(os.path.join(folder_name, 'detailed_action_selection_{}.txt'.format(args['model'])), "a") as w:
            w.write(str(agent.action))
            
    # with open(os.path.join(folder_name, 'reward_list_{}.txt'.format(args['model'])), "w") as w:
    #     w.write(str(rewards))
    with open(os.path.join(folder_name, 'reward_list_{}.txt'.format(args['model'])), "w") as w:
        w.write(str(cumulative_rewards))
    with open(os.path.join(folder_name, 'average_reward_list_{}.txt'.format(args['model'])), "w") as w:
        w.write(str(avg_reward_list))    

def main(args):   
    folder_base = f"result/result_{now.month}_{now.day}_{now.hour}_{now.minute}_{now.second}"
    hyper_params = ''
    if args['folder'] is not None:
        folder_base =  args['folder']
        hyper_params = os.path.join(folder_base,"hyperparameters.json")
    else: 
        os.makedirs(folder_base, exist_ok=True)
    if not os.path.exists(hyper_params):
        if not args['hyperparameters']: 
            print("Not found hyperparameter file")
            return 
        else: 
            hyper_params = args['hyperparameters']
     
    with open(hyper_params,'r') as hp:
        env_config, drl_hyper_params = json.load(hp)   
        
    if env_config["traffic_generator"] == "simulated":
        traffic_gen = PoissonGenerator(size=env_config["num_service"],
                                    avg_requests_per_second=env_config["average_requests"],
                                    max_queue_delay=env_config["rq_timeout"],
                                    max_rq_active_duration=env_config["max_rq_active_duration"])
    elif env_config["traffic_generator"] == "real":
        traffic_gen = RealTraceGenerator(active_duration_stats_file=env_config["active_duration_stats_file"],
                                         arrival_request_stats_file=env_config["arrival_request_stats_file"],
                                         max_queue_delay=env_config["rq_timeout"],
                                         num_services=env_config["num_service"])
        
    if args['observe'] is not None:
        test(args, folder_base, env_config, traffic_gen, drl_hyper_params)
    else:
        with open(os.path.join(folder_base,"hyperparameters.json"), 'w') as file:
            json.dump([env_config, drl_hyper_params], file, indent=4)
        start_time = time.time()
        train(args, folder_base, env_config, traffic_gen, drl_hyper_params)
        training_time = time.time() - start_time
        print(f"Training time: {training_time:.6f} seconds")
        test(args, folder_base, env_config, traffic_gen, drl_hyper_params)       

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Parsing the type of DRL/RL to be tested')
    parser.add_argument('-m', '--model', help='Train DRL/RL', required=True)
    parser.add_argument('-p', '--hyperparameters', help='File contains hyperparameters of env and model')
    parser.add_argument('-o', '--observe', help='Observe a trained DRL/RL')
    parser.add_argument('-f', '--folder', help='Result folder')
    args = vars(parser.parse_args())
    main(args)